services:
  tei-semantic-cache:
    image: ghcr.io/huggingface/text-embeddings-inference:89-1.8
    container_name: litellm-cache
    networks:
      - llm
      - proxy
    volumes:
      - ${DATA_DIR}/tei:/data
    command:
      - --hostname=0.0.0.0
      - --port=4000
      - --hf-token=${HF_TOKEN}
      - --model-id=${SEMANTIC_CACHE_MODEL}
      - --dtype=${TEI_EMBEDDER_DTYPE}
      - --max-batch-tokens=${TEI_EMBEDDER_BATCH}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  tei-embedder:
    image: ghcr.io/huggingface/text-embeddings-inference:89-1.8
    container_name: rag-embedder
    networks:
      - llm
      - proxy
    volumes:
      - ${DATA_DIR}/tei:/data
    command:
      - --hostname=0.0.0.0
      - --port=4000
      - --hf-token=${HF_TOKEN}
      - --model-id=${RAG_EMBEDDING_MODEL}
      - --dtype=${TEI_EMBEDDER_DTYPE}
      - --max-batch-tokens=${TEI_EMBEDDER_BATCH}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  tei-reranker:
    image: ghcr.io/huggingface/text-embeddings-inference:89-1.8
    container_name: rag-reranker
    networks:
      - llm
      - proxy
    volumes:
      - ${DATA_DIR}/tei:/data
    command:
      - --hostname=0.0.0.0
      - --port=4000
      - --hf-token=${HF_TOKEN}
      - --model-id=${RAG_EMBEDDING_MODEL}
      - --dtype=${TEI_RERANKER_DTYPE}
      - --max-batch-tokens=${TEI_RERANKER_BATCH}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
